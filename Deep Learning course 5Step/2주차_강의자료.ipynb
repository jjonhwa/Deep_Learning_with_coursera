{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2주차_01.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 1 : Word Representation</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Word Embeding으로부터 바람직하지 않은 성별, 민족 또는 (학습 알고ㅓ리즘이 때로는 이해할 수 있는 종류의) Bias를 줄이기 위한 것이다.\n",
    "* 각 단어들을 One-Hot Encoding을 하여 하나의 One-hot Vector로 만들어준다.\n",
    "* But, 이 One-hot Vector를 가지고는 각 단어들을 비교할 수 없기 때문에 우리는 다음과 같은 방법으로 처리를 해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2주차_02.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 2 : Why Sequence Model</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 위의 그림처럼 Dense Vector로 표현을 해주며 이는 각 단어들의 특징들을 고려하여 점수로 표현한 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2주차_03.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 3 : Why Sequence Model</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**t-SNE :** 만약 300D으로 주었을 때 이를 2차원 공간에 내장함으로써 시각화하게 해주는 Algorithm이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Embeding이란??**\n",
    "* 단어를 벡터로 표현하는 방법\n",
    "* One-hot Vector는 하나만 1을 가지고 나머지는 전부 0을 가지므로 공간적 낭비를 할 뿐더러 각 단어의 특성을 나타낼 수 없다.\n",
    "* Dense Matrix는 단어들을 표현하기위한 Matrix로 전부 실수의 값을 가진다.\n",
    "* 즉, 단어를 Dense Vector의 형태로 표현하는 것을 Word Embedding이라고 한다!\n",
    "* 또한, Dense Matrix와 Embedding Matrix는 같은 걸 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2주차_04.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 4 : Properties of word embeddings</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analogies Reasoning**\n",
    "* '(Man, Woman)은 (King, ?)인가?'라는 질문에 대하여 우리는 유추를 수행하여 'Queen'이라는 것을 알아낼 수 있다.\n",
    "* 즉, $e_{man} - e_{woman}$과 $e_{king} - ?$의 값을 비교하여 ?를 찾아내는 것이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2주차_05.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 5 : Properties of word embeddings</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 'Cosine Similarity'가 가장 일반적으로 사용되는 유사성이며, 이건 실제로 우리가 앞에서 사용했던 방정식과 동일하다 ($e_{king} - e_{man} + e_{woman} = e_{w}$)\n",
    "* 위의 그림에서 $u$와 $v$가 비슷하다면 내적값은 커질것이다.\n",
    "* 우리가 이를 Cosine Similarity라고 부르는 이유는 $u$와 $v$사이의 각도 때문인데 실제로 각도가 90도이면 유사성은 0이고 각도가 각각 0, 180이면 1, -1을 가진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2주차_06.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 6 : Learning word embeddings</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**위의 그림의 방식으로 NLP에서 Word Embedding을 사용한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2주차_07.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 7 : Learning word embeddings</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 한 단어를 예측할 때, Context와 Target을 다음과 같은 방법들로 Pair할 수 있다.\n",
    "1. 마지막 4개의 단어들로 예측\n",
    "2. 왼쪽과 오른쪽의 4개의 단어들로 예측\n",
    "3. 마지막 단어 하나로 예측\n",
    "4. 가까운 단어 하나로 예측\n",
    "\n",
    "**사실 위에 방법들은 Skip Gram 모델 아이디어이며, 문맥들을 훨씬 단순하게 만들어주며 실제로 매우 잘 작동한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2주차_08.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 8 : Word2Vec</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 단어들을 숫자로 표현해준다, > One-Hot Encoding\n",
    "* 유사도(보통 Euclidean Distance를 사용하거나 Cosine similarity를 사용함)를 만들어주기위하여 Embeding을 사용한다.\n",
    "* Word2Vec은 Word Embedding 중에 하나이며 Skip Gram을 사용한다.(여기서 Window size를 사용하는데 이는 주변의 값들의 개수를 나타낸다. ex)window size = 1 >> 한 단어로부터 왼쪽 오른쪽 한 단어씩)\n",
    "* 위의 그림에서 input값이 우리가 아는 context word이다. 여기서 순서는 input을 넣어준 후 Hidden Layer에 들어가고 이것을 softmax를 이용해 output값으로 나타낸다. 그리고 이것을 target word의 one-hot vector와 비교함으로써 back propagation, front propagation을 반복하여 w1, w2를 결정시키는 것이다. 이때 w1, w2가 word2vec이다.\n",
    "* 위의 식에서는 Hidden Layer에서의 Activation Function은 사용하지 않는다. 즉, Linear Neuron임을 알 수 있다. W와 b만 존재.\n",
    "\n",
    "**Word2Vec을 이용해서 하는 것은??**\n",
    "* 유사도가 없었던 단어들을 유사도가 있는 단어들로 만들어준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2주차_09.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 9 : Word2Vec</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**주된 문제는 계산속도이다.**\n",
    "* 특히, softmax 모델의 경우 확률을 계산할 때마다 많은 단어들을 계산해야한다. 그리고 이것은 위의 그림에서 분모에 해당하는 값을 계산하는 것이 상당히 느리다는 것이 매우느리다. 이를 해결하기위해 Hierarchical Softmax를 사용하는데 이것은 한 번에 10000개를 분류하는 것이 아닌 위의 그림처럼 분류하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**또 하나 중요한것은 context C는 어떻게 sampling 할 것인가?이다.**\n",
    "* Train Set에서 Random Sampling하는 것이다. 하지만 이렇게 하면 of, a, to 같은 단어들이 많이 선택될 것인데 우리는 이것을 원하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2주차_10.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 10 : Negative Sampling</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Negative Sampling은 Skip-Gram 모델과 유사하나 훨씬 더 효율적인 학습 알고리즘이다.\n",
    "* 쌍으로 연결하여 연관성이 있을경우 1, 없을경우 0으로 target값을 준다.\n",
    "* Target값이 1을 가지는 Word를 앞의 Word2Vec에서와 마찬가지로 만들어준다. (Context word를 Sampling하고 +-10개의 단어를 골라서 Target이 1인 행을 만들 수 있다.)\n",
    "* 우리가 Data Set을 만드는 방법은 Context Word를 선택한 후 Target Word를 선택해준다.\n",
    "* Training하는 방법은 target값이 1인 word를 주고 나머지는 Random하게 word를 뽑아서 target을 0으로 주어 학습시킨다. (맞나?)\n",
    "* 실제로 예측할 때는 Context와 word를 입력하여 target값을 알아내어 word를 매칭시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2주차_11.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 11: Negative Sampling</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Negative Sampling은 10000개를 모두 집어넣어 훈련시키는 것이 아니라 target이 1인 값과 target이 0인 '몇'개의 값을 랜덤으로 뽑은 녀석들만 훈련시킨다.\n",
    "* 10000개의 softmax를 하는 것이 아니라 10000개의 binary classification으로 바꾼것이다.\n",
    "\n",
    "**Negative Sampling이라고 한 이유는 ??**\n",
    "* Positive Sample을 하나 주고 여러 Negative Sample을 만들어내기 때문이다.\n",
    "\n",
    "* Negative Sampling을 랜덤으로 뽑을 수도 있지만,,, 경험적으로 가장 좋은 방법은 Heuristic Value를 찾는 것이다.\n",
    "* Heuristic Value는 다음의 식을 이용하여 사용한다.\n",
    "$$ P(w_{i}) = {{f(w_{i})}^{3\\over4} \\over \\sum_{j=1}^{10000} {f(w_{i})}^{3\\over4}} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2주차_12.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 12: GloVe word vectors</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $x_{ij}$는 서로 얼마나 자주 나타는지 혹은 서로 가까운 단어를 캡처하는 수를 나타낸다.\n",
    "* 여기서 우리가 원하는 것은 두 단어가 얼마나 연관성이 있는지를 알고 싶은 것이다. 즉, 위의 식을 최소화하는 것을 의미한다.\n",
    "* 위에서, $f(x_{ij})$는 Weighting term을 의미한다. 즉, $f(x_{ij}) = 0$이면 $x_{ij} = 0$이다. 그리고 log값 역시 0으로 정의한다.\n",
    "* 가중치 $f$는 또한 this, the, a와 같은 말들이 자주 나타나는지 알아낸다. 즉, 이런 단어들에는 가중치를 주지 않는 것이다.\n",
    "* ?? $\\theta$와 $e$가 완전히 대칭적이다, $\\theta_{i}$, $e_{j}$는 완전히 대칭이다. ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2주차_13.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 13: Sentiment Classification</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentiment Classification이란??**\n",
    "* 문자 하나를 보고 그게 좋은 말인지 안 좋은 말인지 분류하는 작업이다.\n",
    "* Word Embedding을 이용하면 작은 규모의 Lable Training Set만 있어도 좋은 Sentiment Classificaiton을 만들 수 있다.\n",
    "* 과제가 있다면 그것은 Not Huge Label Dataset를 가지고 있다는 것이다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2주차_14.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 14: Sentiment Classification</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Embedding Vector들의 평균이나 합계를 구하여 이를 별을 0~5개로 부여해주는 softmax를 통과함으로써 $\\hat{y}$를 구할 수 있다.\n",
    "* 위의 알고리즘의 문제 중 하나는 단어 순서를 무시한다는 것이다.\n",
    "* 예를 들어, $$\"Completely\\space lacking\\space in\\space good\\space taste,\\space good\\space service,\\space good\\space ambience\"$$라는 단어가 있을경우, 안 좋은 표현임에도 불구하고 좋은 점수를 가지게 될 것이다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2주차_15.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 15: Sentiment Classification</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**그래서 위와 같은 평균이나 합계가 아닌 각 Embedding Vector를 RNN Model에 넣은 후 Many-to-One의 모양으로 $\\hat{y}$를 출력하도록 한다.**\n",
    "\n",
    "**단어 사전에 없는 단어가 등록되어 있더라고 Word Embedding을 training할 때 많은 수의 단어들로 했다면 여전히 옳게 해석해 줄 것이다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2주차_16.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 16: Debiasing word embeddings</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Word Embedding을 하다보면 각 대응 쌍을 위와 같이 편견이 있는 모양으로 만들어 줄 수 있다. 이것은 Word Embedding의 Bias 문제로 이를 줄이기 위해 아직도 많은 연구와 노력이 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2주차_17.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 17: Debiasing word embeddings</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Embedding과정에서 성 편견 같은 여러 Bias들이 제거되는 몇 가지 아이디어가 있다.**\n",
    "1. Identify bias direction ($\\mathbf{e}_{he} - \\mathbf{e}_{she}$와 같이 계산하여 이들의 평균을 낸다.)\n",
    "2. Neutralize : 정의적이지 않은 모든 단어들의 편견을 제거해준다.\n",
    "    * Grandmother, boy, she와 같은 단어들은 gender가 포함되어 있지만 babysitter나 doctor와 같은 단어들은 중립이 되도록 만들어준다.\n",
    "    * Bias축의 값을 0으로 만들어준다.\n",
    "3. Equalize pairs.\n",
    "    * 예를 들면, (할머니, 할아버지), (소녀, 소년)의 차이점이 유일하게 성별이 되도록 하는 것이다.\n",
    "    * Bias축의 값을 0으로 만들었을 때 (baby sitter와 doctor라 하자) (할머니, 할아버지)와 (소녀, 소년) 각 쌍과의 거리가 차이나지 않게 해주는 것이다.\n",
    "    * 선형 대수학 단계가 있다 >> 이것은 (할머니, 할아버지)라는 단어를 bias = 0으로부터 즉 non-bias 축으로부터 같은 거리만큼 떨어지도록 만들어주는 것이다.\n",
    "    \n",
    "**그렇다면 Neutralize할 단어들은 어떻게 선택하는가?**\n",
    "* 대부분의 단어가 정의적이지 않으며 정의적이지 않은 단어들은 몇 개 없으므로 대부분 손으로 직접 뽑는 것이 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
