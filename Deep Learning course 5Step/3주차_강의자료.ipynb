{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/3주차_01.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 1 : Basic Models/center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequence to Sequence Model의 모양을 나타내고 있다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/3주차_02.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 2 : Basic Models</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image Captioning을 나타내고 있으며 이미지로부터 그 이미지에 해당하는 문장을 뽑아내는 모델이다.**\n",
    "* 이미지로부터 CNN을 실행시킨후 Flatten Vector에서 Softmax로 가지 않고 RNN으로 들어와 문장을 만들어간다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/3주차_03.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 3 : Picking the most likely sentence</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Language Model의 경우 각 단어의 확률을 계산하여 단어를 선택했던 Model이다. (Language Modeling이란 잘 형성된 문장에 높은 확률을 부여하여 배우는 언어 모델이다.)\n",
    "* 이에 반해, Machine Translation은 encoding부분과 decoding부분으로 나뉘어져 있다.\n",
    "* 이 때, Decoding 부분이 Language Model과 유사하며 그 차이는 input으로 들어오는 것 0이냐 인코딩된 네트워크냐의 차이이다.\n",
    "* Machine Translation의 경우 어떤 문장의 확률을 모델링하는 것이 아닌 투입되는 문장에 대한 출력 문장의 확률을 모델링한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/3주차_04.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 4 : Picking the most likely sentence</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 랜덤으로 Sampling하는 것이 아닌 $x$에 대한 조건부 확률이 가장 높은 문장을 찾는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/3주차_05.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 5 : Picking the most likely sentence</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**왜 Greedy Search는 안될까?**\n",
    "* 우리가 원하는 것은  전체 단어의 공동 확률을 극대화하는 것이다.\n",
    "* But, greedy search는 각 단어마다의 확률이 가장 높은 것을 찾는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/3주차_06.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 6 : Beam Search</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* greedy search와 같은 방향으로 가지만 Beam Search Algorithm은 parameter $B$를 가진다. (이는 'B=3'일 때 3가지 단어들의 확률을 평가한다는 뜻으로 greedy search보다 더 넓게 단어들을 평가할 수 있다.\n",
    "* 확률값은 $P(y^{<1>},y^{<2>}|x) = P(y^{<1>})P(y^{<2>}|x, y^{<1})$으로 구해준다.\n",
    "* 다음 각 단어 대하여 10000개를 나열하는데 거기서 각 3개가 아닌 그 중 (즉, 30000)에서 확률이 높은 'B'개를 선택한다.(이 때, B = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/3주차_07.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 7 : Refinements to beam search</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**길이 정규화**\n",
    "* $\\mathbf{arg\\space max_y} \\prod_{t=1}^{T_y}P(y^{<t>}|x, y^{<1>}, \\dots , y^{<t-1>})$는 앞서 본 조건부 확률을 나타낸다.\n",
    "* 위의 식을 정규화하기 위하여 Log를 사용한다. $\\mathbf{arg\\space max_y} \\prod_{t=1}^{T_y}logP(y^{<t>}|x, y^{<1>}, \\dots , y^{<t-1>})$\n",
    "* 또한, Max값을 구하는 것이 아닌 평균을 사용할 수도 있다. 평균을 사용하는 것은 긴 번역을 치루는 것에 대한 위험을 상당히 감소시키는 효과를 볼 수 있다.\n",
    "* 단어의 수로 나누는 것 대신에 가끔은 $T_{y}^{\\alpha}$로 나누어 주는데 이 떄 흔히 $\\alpha$를 0.7로 주면 이론적 근거는 없지만 실제로 효과가 있음을 알아내었다. 그래서 $\\alpha$값을 하이퍼 파라미터로서 튜닝을 함으로써 더 좋은 효과를 얻을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beam width $B$는 어떻게 결정할까??**\n",
    "* $B$가 클수록 우리가 발견할 수 있는 문장이 더 좋다.\n",
    "* $B$가 클수록 더 많은 계산 비용이 든다.\n",
    "* $B$가 클수록 더 많은 경우의 수들을 고려하게되어 우리는 더 나은 결과를 얻는 경향이 있다.\n",
    "* 하지만, 메모리 요구사항도 증가하며 속도 역시 느려진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/3주차_08.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 8 : Error analysis in beam search</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 여기서 Error Analysis란 사람이 번역한 결과물을 $\\hat{y}$라고 할 때 $P(y^{*}|x)$와 $P(\\hat{y}|x)$를 비교한 것이다.\n",
    "* Case 1 : $P(y^{*}|x) > P(\\hat{y}|x)$이면 Beam Search Algorithm에 문제가 있는 것으로 알 수 있다.\n",
    "* Case 2 : $P(y^{*}|x) <= P(\\hat{y}|x)$이면 RNN에 문제가 있다는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/3주차_09.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 9 : Attention Model Intuition</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**위의 그림으로부터 Attention Model의 Intuition을 알 수 있다.**\n",
    "\n",
    "**Attention Model을 가장 영향력있는 모델 중 하나이다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/3주차_10.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 10 : Attention Model</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\mathbf{a}^{<{t^\\prime}>}$는 font propagation과 back propagation을 모두 포함하는 것을 의미한다.\n",
    "* $\\alpha$값들은 $c$에 주는 가중치를 나타낸다.\n",
    "* 가중치들의 합은 1이 될 것이다.\n",
    "* $c^{<1>}$의 값은 $\\sum_{t^{\\prime}}\\alpha^{<1,t^{\\prime}>}\\mathbf{a}^{<t^{\\prime}>}$이 된다.\n",
    "* $\\alpha^{<t,t^{\\prime}>}$는 $\\mathbf{a}^{<t^{\\prime}>}$이 $y^{<t>}$에 미치는 attention의 양이다.\n",
    "* Attention Model을 사용하면 한 번에 한 단어씩 번역을 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/3주차_11.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 11 : Attention Model</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**그렇다면 가중치를 어떻게 결정할 것인가?**\n",
    "* 위의 그림의 식을 사용한다.\n",
    "* 또한, 위의 식의 $e^{<t,t^{\\prime}>}$을 결정할 때는 small neural network를 사용하게된다.\n",
    "    * 이것은 $s^{<t-1>}$과 $\\mathbf{a}^{t^{\\prime}}$의 영향을 받는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speech Recognition, Trigger Word Detection...좀 더"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/3주차_12.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 12 : Speech recognition</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CTC 비용 기능의 기본 규칙은 'blank'로 구분되지 않은 반복된 문자를 축소하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/3주차_13.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 13 : Trigger Word Detection</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Trigger Word가 나올 때 1을 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
