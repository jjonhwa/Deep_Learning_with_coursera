{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/1주차_01.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 1 : Why Sequence Model</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Speech Recognition         : 음성 >> Text\n",
    "2. Music Generation           : 몇개의 음 + 장르 >> 음악추천\n",
    "3. Sentiment Classification   : 문자열, 문장 >> 점수 \n",
    "4. DNA Sequence Analysis      : 염기서열\n",
    "5. Machine Translation        : 언어 >> 다른 언어\n",
    "6. Video Activity Recognition : 비디오 >> 종목\n",
    "7. Name Entity Recognition    : 문장에서 이름만 추춣"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/1주차_02.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 2 : Notation</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예들 들어, 10,000개의 어휘사전(빈도수 상위 10,000개 혹은 온라인 사전에서 가져와 사용하도록 한다.)이 있다고 할 때, 이 사전에 따라 one-hot encoding을 한다.\n",
    "\n",
    "**Notaion :**\n",
    "- 위첨자 $[l]$는 $l^{th}$ layer와 관련된 객체를 나타낸다.\n",
    "- 위첨자 $(i)$는 $i^{th}$ example과 관련된 객체를 나타낸다.\n",
    "- 위첨자 $\\langle t \\rangle$는 $t^{th}$ time-step에서의 객체를 나타낸다.\n",
    "- 아래첨자 $i$는 한 벡터에서 $i^{th}$ entry를 나타낸다.\n",
    "- $T_{x}$는 Sequence의 길이를 나타낸다.\n",
    "\n",
    "사전에 없는 단어가 나올 경우 새로운 토큰 $<UNK>$를 만들던지 혹은 가짜 단어를 만들어 줄 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/1주차_03.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 3 : RNN Model</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**왜 Basic NN으로는 하지 않고 RNN으로 하는가?**\n",
    "1. 예제가 바뀔 떄마다 input, output값이 바뀌기 때문에.\n",
    "2. 서로 다른 위치에서 학습된 feature가 sharing되지 않기 때문에."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/1주차_04.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 4 : RNN Model</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**순환이란?? What is Recurrent**\n",
    "* 예를 들어, $x_2$를 이용해 $y_2$를 예측한다고 할 떄, $x_1$연산의 일부를 사용하는 것을 말한다. 즉, Prediction을 구할 때 input값만으로 예측하는 것이 아니라, 앞의 것들을 사용한다는 것을 의미함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/1주차_05.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 5 : RNN Model</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/1주차_06.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 6 : RNN-Cell Model</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "식에 대한 내용은 넘어가도록 한다. 가중치의 경우 입력값으로 입력해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/1주차_07.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 7 : BackPropagation Through Time</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BackPropagation 같은 경우 프로그램에서 자동으로 처리해주지만 알고 넘어가도록 하자.\n",
    "\n",
    "Back의 경우 Front와 큰 차이점은 없으며 Loss Function을 구해준 후 단순히 반대방향으로 계싼 혹은 구문 분석만 해주면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/1주차_08.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 8 :Different Types of RNNs</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Many-to-Many : Sentiment Classification\n",
    "2. One-to-Many  : Music Generation\n",
    "3. Many-to-Many (input != output) : Machine Translation\n",
    "4. Many-to-Many (input = output)  : Name Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/1주차_09.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 9 :Language Model & Sequence Generation</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Speech Recognition System이 각 단어들을 선택하는 기준은 각 단어들의 확률이 얼마인지를 계산해 알려주는 Lanugage Model을 사용하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/1주차_10.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 10 :Language Model & Sequence Generation</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $<EOS>$는 문장의 마지막에 위치한 토큰이다.\n",
    "* 위의 그림에서 보는 것 처럼 조건부 확률값으로 $\\hat{y}^{<t>}$값을 구하여 Loss Function값을 구해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing Gradient With RNNs\n",
    "**기본적인 RNN Algorithm의 문제는 Vanishing Gradient의 문제이다. but 여기서는 Exploding Gradient에 대한 설명을 했다.....뭐지?**\n",
    "* 경사 폭발을 하게되면 Parameters가 Messed Up된다.\n",
    "* Parameters의 Messed Up으로 인하여 그 값이 NaN을 가질 수도 있으며, 숫자가 아닌 값을 가지기도 한다.\n",
    "* Exploding의 경우 Gradient Clipping을 하게되는데 이는 Gradient Vector를 살펴보는 것으로서 threshold보다 클 경우 re-scale해주어 너무 크지 않도록 만들어준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/1주차_11.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 11 :Gated Recurrent Unit</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GRU에서는 $c$라는 새로운 변수를 갖는다.\n",
    "* $c$는 Memory cell을 나타내는데 이는 'cat'이라는 단어가 단수인지 복수인지 기억할 수 있는 작은 memory를 제공하는 것이다.\n",
    "* 또한 $c^{<t>}$는 $\\tilde{c}^{<t>}$로 덮어씌어질 것이다.\n",
    "* $\\Gamma_{u}$는 $c^{<t>}$를 Update할 것인지 결정한다.\n",
    "* $\\Gamma_{u}$거 0이면 $c^{<t>}$를 그대로 보존한다.\n",
    "* $\\Gamma_{u}$를 사용하는 이유는 sigmoid를 사용함으로써 Gradient Vanishing문제에 많은 도움을 주기 때문이다.\n",
    "* 또한, GRU에서는 $\\Gamma_{r}$을 연관성에 대한 논리로 $c^{<t-1>}$과 $c^{<t>}$의 관련성을 알려주는데 이 역시 Gradient Vanishing의 문제를 다루기위해 사용한다고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/1주차_12.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 12 :LSTM ( Long Short Term Memory )</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRU vs LSTM\n",
    "* LSTM은 GRU보다 많은 Gate를 사용함으로써 보다 강력하고 효과적이다.\n",
    "* 이에반해, GRU는 적은 Gate를 사용함으로써 좀 더 Simplified하다고 볼 수 있다.\n",
    "* LSTM역시 Vanishing Gradient 이론을 쫒는다.\n",
    "* GRU에서 $a^{<t>}$ = $c^{t}>$인 것에 반하여 LSTM에서는 $a^{<t>}$ = $\\Gamma_o *$ tanh($c^{<t>}$)를 사용한다.\n",
    "* 또한, $c^{<t>}$를 구하는데 있어서 ($1 - \\Gamma_u$)가 아닌 $\\Gamma_f$를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/1주차_13.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 13 :LSTM ( Long Short Term Memory )</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그림처럼 흘러간다. 참고만 하도록 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/1주차_14.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 14 :Bidirectional RNN ( BRNN )</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RNN이 앞의 값들에만 영향을 받았다면 Bidirectional RNN은 양방향의 영향을 모두 받는다.\n",
    "* 단점으로는, 어디에서든 예측을 하기 앞서 전체 데이터 Sequence가 필요하다는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/1주차_15.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Figure 15 :Deep RNNs</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{a}^{[1]}$은 1번째 레이어를 뜻하는 것으로 지금까지 하나의 레이어층으로 되어있는 것만 봐온 반면 위의 그림은 Deep RNN의 경우 여러개의 layer로 구성되어 있음을 보여준다. 또한 각, $y^{<t>}$을 구할 때 항상 같은 RNN만을 사용하는 것이 아니라 서로 다른 것들을 사용할 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
