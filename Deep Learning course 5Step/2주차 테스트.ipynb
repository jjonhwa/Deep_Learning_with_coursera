{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2주차_1번.png\" style=\"width:800px;height:150px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**우리가 10000개의 단어를 포함하는 word embedding을 배운다고 가정하자. 그러면, embedding vector는 10000차원을 가지며 평균과 분산의 모든 범위를 capture할 수 있다.**\n",
    "* False, embedding vector는 우리가 선택한 특징이 128개라고 가정했을 떄, 128차원을 가진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2주차_2번.png\" style=\"width:800px;height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**t-SNE란 무엇인가?**\n",
    "* 차원 축소의 역할을 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2주차_3번.png\" style=\"width:800px;height:400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**방대한 양의 텍스트에 의해 사전에 학습된 word embedding을 다운로드 한다고 가정하자. 우리는 이 word embedding을 이용하여 언어 인식을 위한 RNN을 후련하는데 사용한다. 누군가가 행복항지 인식하는 언어작업을 위해 작은 훈련 set을 사용할 것이다. 위와 같을 때, '황홀하다'라는 단어가 작은 training set에 나타나지 않더라도 우리는 \"I'm ecstatic\"을 'y=1'로 구분할 수 있을 것이다.**\n",
    "* \"I'm ecstatic\"은 흔한 단어가 아니므로 small dataset에서 훈련을 시켜주지 않는다면 알 수 없을 것이다. <<라고 처음에 생각함.\n",
    "* But, word vectos는 놀라운 힘으로 활홀함을 1로 나타내 줄 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2주차_4번.png\" style=\"width:700px;height:180px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**좋은 word embedding이라고 생각되는 것은 무엇인가?**\n",
    "* 같은 부류의 category들을 묶어주고 순서도 맞춰준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2주차_5번.png\" style=\"width:800px;height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E가 embedding Matrix라고 하자, $o_{1234}$가 1234번째에 대응하는 단어의 one-hot vector라고 하자. 그러면 1234번째 단어의 embedding을 얻었을 때 왜 우리는 파이썬에서 E * $o_{1234}$라고 불러 오지 않는가?**\n",
    "* 파이썬에서는 위에 식대로 불러오면 된다. << 라고 생각했었지만...\n",
    "* but, 3번 : $<UNK>$를 다루지 않기 때문에 위의 식으로 파이썬에 불러오지 않는 것이다. << 라고 생각했었찌만....\n",
    "* But, 계산적인 낭비로 인해서 사용하지 않는다!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2주차_6번.png\" style=\"width:800px;height:150px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**word embedding을 할 때, 우리는 $P(target|context)$를 인위적으로 만든다. 우리는 artificial prediction task를 잘 수행하지 못해도 괜찮다; 이 과제의 부산물은 우리가 word embeddings에 유용한 set을 배우는 것이다.**\n",
    "* True : artificial prediction task를 잘하지 못하더라도 이후에 prediction task에서 잘 할수 있도록 유용한 set을 얻는 것이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2주차_7번.png\" style=\"width:800px;height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**word2vec 알고리즘에서 우리는 $P(t∣c)$를 추론한다. $t$는 target word이고 c는 context word이다. training set으로 부터 어떻게 t와 c를 선택해야 할까? 가장 옳은 정답을 선택하라.**\n",
    "* c와 t는 원래 모든 sequence에서 선택되어지는 것인데 여기에서 가장 옳은 정답은 가까운 단어들로부터 t와 c를 선택하는 것!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2주차_8번.png\" style=\"width:800px;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10000개의 단어가 있고, 500차원의 word embedding을 학습했다. word2vec모델은 다음의 softmax 함수를 사용한다. 옳은 것은 무엇인가?**\n",
    "* 잘 모르겠음....\n",
    "* 둘 다 500차원이다.\n",
    "* 훈련 후에 우리는 t와 c가 같을 때 두 개가 매우 비슷할 것을 기대한다. << 라고 생각했었는데 아니었음.\n",
    "* 둘 다 최적화 알고리즘 즉 Adam 이나 경사 하강을 사용하여 train 되어진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2주차_9번.png\" style=\"width:800px;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10000개의 단어가 있고, 500차원의 word embedding을 학습했다. GloVe model을 최소화 하는 객체는 다음과 같다. 다음 중 옳은 것은 무엇인가?**\n",
    "* 잘 모르겠음..\n",
    "* $X_{ij}$는 단어 i의 context에서 word j 가 나타난 횟수이다.\n",
    "* weiging function은 $X_{ij}$가 0일 때 0을 갖는다.\n",
    "* 0으로 초기화가 되는 것이 아니다.(왜냐면 c, t랑 매치되는 것이 i, j이므로 이를 바탕으로 생각해 보았을 때, c, t는 우리가 알고 싶은 단어들이기 때문에 0이 아닌, random하게 initialize 해주어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2주차_10번.png\" style=\"width:800px;height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**우리는 $m_{1}$개의 단어를 텍스트 데이터 집합으로 사용하여 word embedding을 훈련했다. 우리는 언어 작업에 있어서 이 word embedding사용을 고려하고 있으며 우리는 별도의 라벨로 이루어진 $m_{2}$ 개의 단어 dataset를 사용할 것이다. word embdding을 사용하는 것은 trasfer learning의 형태임을 명심하자. 어떤 것이 이 상황에 더 도움이 될 것으로 생각되는가?**\n",
    "* $m_{1} >> m_{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
